Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
15/05/17 18:55:36 INFO SparkContext: Running Spark version 1.3.1
15/05/17 18:55:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/05/17 18:55:36 INFO SecurityManager: Changing view acls to: Miguel
15/05/17 18:55:36 INFO SecurityManager: Changing modify acls to: Miguel
15/05/17 18:55:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(Miguel); users with modify permissions: Set(Miguel)
15/05/17 18:55:36 INFO Slf4jLogger: Slf4jLogger started
15/05/17 18:55:36 INFO Remoting: Starting remoting
15/05/17 18:55:36 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.1.109:57075]
15/05/17 18:55:36 INFO Utils: Successfully started service 'sparkDriver' on port 57075.
15/05/17 18:55:36 INFO SparkEnv: Registering MapOutputTracker
15/05/17 18:55:36 INFO SparkEnv: Registering BlockManagerMaster
15/05/17 18:55:36 INFO DiskBlockManager: Created local directory at /var/folders/d6/kwsrvx8d3y9f6xztyntztcd40000gn/T/spark-9ea5291c-8351-47bd-9e9d-90b8a7be1b74/blockmgr-594dc682-2f01-466a-8039-ab6b444c976e
15/05/17 18:55:36 INFO MemoryStore: MemoryStore started with capacity 265.1 MB
15/05/17 18:55:36 INFO HttpFileServer: HTTP File server directory is /var/folders/d6/kwsrvx8d3y9f6xztyntztcd40000gn/T/spark-cf934e59-1919-4ad3-bfcd-d71017338854/httpd-501c7400-8c3d-4407-8a68-b61e4d28d33e
15/05/17 18:55:36 INFO HttpServer: Starting HTTP Server
15/05/17 18:55:36 INFO Server: jetty-8.y.z-SNAPSHOT
15/05/17 18:55:37 INFO AbstractConnector: Started SocketConnector@0.0.0.0:57076
15/05/17 18:55:37 INFO Utils: Successfully started service 'HTTP file server' on port 57076.
15/05/17 18:55:37 INFO SparkEnv: Registering OutputCommitCoordinator
15/05/17 18:55:37 INFO Server: jetty-8.y.z-SNAPSHOT
15/05/17 18:55:37 WARN AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:437)
	at sun.nio.ch.Net.bind(Net.java:429)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.spark-project.jetty.server.Server.doStart(Server.java:293)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:199)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:209)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:209)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1837)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1828)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:209)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:120)
	at org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:309)
	at org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:309)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:309)
	at com.hadooparchitecturebook.spark.dedup.SparkDedupExecution$.main(SparkDedupExecution.scala:28)
	at com.hadooparchitecturebook.spark.dedup.SparkDedupExecution.main(SparkDedupExecution.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/05/17 18:55:37 WARN AbstractLifeCycle: FAILED org.spark-project.jetty.server.Server@636e8cc: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:437)
	at sun.nio.ch.Net.bind(Net.java:429)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.spark-project.jetty.server.Server.doStart(Server.java:293)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:199)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:209)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:209)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1837)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1828)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:209)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:120)
	at org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:309)
	at org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:309)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:309)
	at com.hadooparchitecturebook.spark.dedup.SparkDedupExecution$.main(SparkDedupExecution.scala:28)
	at com.hadooparchitecturebook.spark.dedup.SparkDedupExecution.main(SparkDedupExecution.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
15/05/17 18:55:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
15/05/17 18:55:37 INFO Server: jetty-8.y.z-SNAPSHOT
15/05/17 18:55:37 WARN AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4041: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:437)
	at sun.nio.ch.Net.bind(Net.java:429)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.spark-project.jetty.server.Server.doStart(Server.java:293)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:199)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:209)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:209)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1837)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1828)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:209)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:120)
	at org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:309)
	at org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:309)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:309)
	at com.hadooparchitecturebook.spark.dedup.SparkDedupExecution$.main(SparkDedupExecution.scala:28)
	at com.hadooparchitecturebook.spark.dedup.SparkDedupExecution.main(SparkDedupExecution.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/05/17 18:55:37 WARN AbstractLifeCycle: FAILED org.spark-project.jetty.server.Server@1433046b: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:437)
	at sun.nio.ch.Net.bind(Net.java:429)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.spark-project.jetty.server.Server.doStart(Server.java:293)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:199)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:209)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:209)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1837)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1828)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:209)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:120)
	at org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:309)
	at org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:309)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:309)
	at com.hadooparchitecturebook.spark.dedup.SparkDedupExecution$.main(SparkDedupExecution.scala:28)
	at com.hadooparchitecturebook.spark.dedup.SparkDedupExecution.main(SparkDedupExecution.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
15/05/17 18:55:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
15/05/17 18:55:37 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
15/05/17 18:55:37 INFO Server: jetty-8.y.z-SNAPSHOT
15/05/17 18:55:37 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4042
15/05/17 18:55:37 INFO Utils: Successfully started service 'SparkUI' on port 4042.
15/05/17 18:55:37 INFO SparkUI: Started SparkUI at http://192.168.1.109:4042
15/05/17 18:55:37 INFO SparkContext: Added JAR file:/Users/Miguel/Git/hadoop-arch-book/ch05-processing-patterns/dedup/spark/./target/BellCode.jar at http://192.168.1.109:57076/jars/BellCode.jar with timestamp 1431885337358
15/05/17 18:55:37 INFO Executor: Starting executor ID <driver> on host localhost
15/05/17 18:55:37 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.1.109:57075/user/HeartbeatReceiver
15/05/17 18:55:37 INFO NettyBlockTransferService: Server created on 57077
15/05/17 18:55:37 INFO BlockManagerMaster: Trying to register BlockManager
15/05/17 18:55:37 INFO BlockManagerMasterActor: Registering block manager localhost:57077 with 265.1 MB RAM, BlockManagerId(<driver>, localhost, 57077)
15/05/17 18:55:37 INFO BlockManagerMaster: Registered BlockManager
15/05/17 18:55:37 INFO MemoryStore: ensureFreeSpace(181288) called with curMem=0, maxMem=278019440
15/05/17 18:55:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.0 KB, free 265.0 MB)
15/05/17 18:55:37 INFO MemoryStore: ensureFreeSpace(25913) called with curMem=181288, maxMem=278019440
15/05/17 18:55:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 25.3 KB, free 264.9 MB)
15/05/17 18:55:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:57077 (size: 25.3 KB, free: 265.1 MB)
15/05/17 18:55:37 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/05/17 18:55:37 INFO SparkContext: Created broadcast 0 from hadoopFile at SparkDedupExecution.scala:31
15/05/17 18:55:37 INFO FileInputFormat: Total input paths to process : 2
15/05/17 18:55:37 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/05/17 18:55:37 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/05/17 18:55:37 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/05/17 18:55:37 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/05/17 18:55:37 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/05/17 18:55:37 INFO SparkContext: Starting job: saveAsTextFile at SparkDedupExecution.scala:48
15/05/17 18:55:37 INFO DAGScheduler: Registering RDD 1 (map at SparkDedupExecution.scala:38)
15/05/17 18:55:37 INFO DAGScheduler: Got job 0 (saveAsTextFile at SparkDedupExecution.scala:48) with 2 output partitions (allowLocal=false)
15/05/17 18:55:37 INFO DAGScheduler: Final stage: Stage 1(saveAsTextFile at SparkDedupExecution.scala:48)
15/05/17 18:55:37 INFO DAGScheduler: Parents of final stage: List(Stage 0)
15/05/17 18:55:37 INFO DAGScheduler: Missing parents: List(Stage 0)
15/05/17 18:55:37 INFO DAGScheduler: Submitting Stage 0 (MapPartitionsRDD[1] at map at SparkDedupExecution.scala:38), which has no missing parents
15/05/17 18:55:37 INFO MemoryStore: ensureFreeSpace(3368) called with curMem=207201, maxMem=278019440
15/05/17 18:55:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 264.9 MB)
15/05/17 18:55:37 INFO MemoryStore: ensureFreeSpace(2424) called with curMem=210569, maxMem=278019440
15/05/17 18:55:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.4 KB, free 264.9 MB)
15/05/17 18:55:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:57077 (size: 2.4 KB, free: 265.1 MB)
15/05/17 18:55:37 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0
15/05/17 18:55:37 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:839
15/05/17 18:55:37 INFO DAGScheduler: Submitting 2 missing tasks from Stage 0 (MapPartitionsRDD[1] at map at SparkDedupExecution.scala:38)
15/05/17 18:55:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
15/05/17 18:55:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1404 bytes)
15/05/17 18:55:38 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1408 bytes)
15/05/17 18:55:38 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
15/05/17 18:55:38 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/05/17 18:55:38 INFO Executor: Fetching http://192.168.1.109:57076/jars/BellCode.jar with timestamp 1431885337358
15/05/17 18:55:38 INFO Utils: Fetching http://192.168.1.109:57076/jars/BellCode.jar to /var/folders/d6/kwsrvx8d3y9f6xztyntztcd40000gn/T/spark-fad5bfab-9c36-4df4-9eac-d56bb22f902b/userFiles-5072db6e-2d40-4d0d-b501-a4bbb891965c/fetchFileTemp5939684223475536350.tmp
15/05/17 18:55:38 INFO Executor: Adding file:/var/folders/d6/kwsrvx8d3y9f6xztyntztcd40000gn/T/spark-fad5bfab-9c36-4df4-9eac-d56bb22f902b/userFiles-5072db6e-2d40-4d0d-b501-a4bbb891965c/BellCode.jar to class loader
15/05/17 18:55:38 INFO HadoopRDD: Input split: file:/Users/Miguel/Git/hadoop-arch-book/ch05-processing-patterns/dedup/spark/ded_input/ded_10.txt:0+87
15/05/17 18:55:38 INFO HadoopRDD: Input split: file:/Users/Miguel/Git/hadoop-arch-book/ch05-processing-patterns/dedup/spark/ded_input/ded_100000.txt:0+1367876
15/05/17 18:55:38 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2004 bytes result sent to driver
15/05/17 18:55:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 414 ms on localhost (1/2)
15/05/17 18:55:38 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2004 bytes result sent to driver
15/05/17 18:55:38 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 566 ms on localhost (2/2)
15/05/17 18:55:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/05/17 18:55:38 INFO DAGScheduler: Stage 0 (map at SparkDedupExecution.scala:38) finished in 0.632 s
15/05/17 18:55:38 INFO DAGScheduler: looking for newly runnable stages
15/05/17 18:55:38 INFO DAGScheduler: running: Set()
15/05/17 18:55:38 INFO DAGScheduler: waiting: Set(Stage 1)
15/05/17 18:55:38 INFO DAGScheduler: failed: Set()
15/05/17 18:55:38 INFO DAGScheduler: Missing parents for Stage 1: List()
15/05/17 18:55:38 INFO DAGScheduler: Submitting Stage 1 (MapPartitionsRDD[4] at saveAsTextFile at SparkDedupExecution.scala:48), which is now runnable
15/05/17 18:55:38 INFO MemoryStore: ensureFreeSpace(128408) called with curMem=212993, maxMem=278019440
15/05/17 18:55:38 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 125.4 KB, free 264.8 MB)
15/05/17 18:55:38 INFO MemoryStore: ensureFreeSpace(77221) called with curMem=341401, maxMem=278019440
15/05/17 18:55:38 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 75.4 KB, free 264.7 MB)
15/05/17 18:55:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:57077 (size: 75.4 KB, free: 265.0 MB)
15/05/17 18:55:38 INFO BlockManagerMaster: Updated info of block broadcast_2_piece0
15/05/17 18:55:38 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:839
15/05/17 18:55:38 INFO DAGScheduler: Submitting 2 missing tasks from Stage 1 (MapPartitionsRDD[4] at saveAsTextFile at SparkDedupExecution.scala:48)
15/05/17 18:55:38 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
15/05/17 18:55:38 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, PROCESS_LOCAL, 1110 bytes)
15/05/17 18:55:38 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, localhost, PROCESS_LOCAL, 1110 bytes)
15/05/17 18:55:38 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)
15/05/17 18:55:38 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)
15/05/17 18:55:38 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/05/17 18:55:38 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/05/17 18:55:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/05/17 18:55:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/05/17 18:55:38 INFO FileOutputCommitter: Saved output of task 'attempt_201505171855_0001_m_000001_3' to file:/Users/Miguel/Git/hadoop-arch-book/ch05-processing-patterns/dedup/spark/ded_output/_temporary/0/task_201505171855_0001_m_000001
15/05/17 18:55:38 INFO FileOutputCommitter: Saved output of task 'attempt_201505171855_0001_m_000000_2' to file:/Users/Miguel/Git/hadoop-arch-book/ch05-processing-patterns/dedup/spark/ded_output/_temporary/0/task_201505171855_0001_m_000000
15/05/17 18:55:38 INFO SparkHadoopMapRedUtil: attempt_201505171855_0001_m_000001_3: Committed
15/05/17 18:55:38 INFO SparkHadoopMapRedUtil: attempt_201505171855_0001_m_000000_2: Committed
15/05/17 18:55:38 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 1828 bytes result sent to driver
15/05/17 18:55:38 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 1828 bytes result sent to driver
15/05/17 18:55:38 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 123 ms on localhost (1/2)
15/05/17 18:55:38 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 123 ms on localhost (2/2)
15/05/17 18:55:38 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/05/17 18:55:38 INFO DAGScheduler: Stage 1 (saveAsTextFile at SparkDedupExecution.scala:48) finished in 0.125 s
15/05/17 18:55:38 INFO DAGScheduler: Job 0 finished: saveAsTextFile at SparkDedupExecution.scala:48, took 0.859001 s
